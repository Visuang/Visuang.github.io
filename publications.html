
<head>
    <meta charset="utf-8">
    <title>TVI ¬∑ Resources</title>
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <meta content="" name="keywords">
    <meta content="" name="description">

    <!-- Favicon -->
    <link href="img/logo_fav.png" rel="icon">

    <!-- Google Web Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Heebo:wght@400;500;600&family=Nunito:wght@600;700;800&family=Pacifico&display=swap" rel="stylesheet">

    <!-- Icon Font Stylesheet -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.0/css/all.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.4.1/font/bootstrap-icons.css" rel="stylesheet">

    <!-- Libraries Stylesheet -->
    <link href="lib/animate/animate.min.css" rel="stylesheet">
    <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">
    <link href="lib/tempusdominus/css/tempusdominus-bootstrap-4.min.css" rel="stylesheet" />

    <!-- Customized Bootstrap Stylesheet -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Template Stylesheet -->
    <link href="css/style.css" rel="stylesheet">
    <style>
        .zoom-image {
            width: 100%; /* ÂõæÁâáÂç†Êª°ÂÆπÂô®ÂÆΩÂ∫¶ */
            height: auto; /* È´òÂ∫¶Ëá™ÈÄÇÂ∫î */
            transition: transform 0.4s ease; /* ËÆæÁΩÆÂèòÊç¢ÁöÑËøáÊ∏°ÊïàÊûú */
        }
    
        /* Èº†Ê†áÊÇ¨ÂÅúÂú®ÂõæÁâáÂÆπÂô®‰∏äÊó∂Â∫îÁî®ÁöÑÊîæÂ§ßÊïàÊûú */
        .col-lg-3:hover .zoom-image,
        .col-sm-3:hover .zoom-image {
            transform: scale(3); /* Èº†Ê†áÊÇ¨ÂÅúÊó∂Â∞ÜÂõæÁâáÊîæÂ§ß20% */
        }
    
        .divider {
            border: none;
            border-top: 1px solid #141313; /* ÁÅ∞Ëâ≤ÂàÜÂâ≤Á∫ø */
            margin: 10px 0; /* ËÆæÁΩÆ‰∏ä‰∏ãÈó¥Ë∑ù */
        }
    </style>
    
</head>

<body>
    <div class="container-xxl bg-white p-0">
        <!-- Spinner Start -->
        <div id="spinner" class="show bg-white position-fixed translate-middle w-100 vh-100 top-50 start-50 d-flex align-items-center justify-content-center">
            <div class="spinner-border text-primary" style="width: 3rem; height: 3rem;" role="status">
                <span class="sr-only">Loading...</span>
            </div>
        </div>
        <!-- Spinner End -->



        <!-- Navbar & Hero Start -->
        <div class="container-xxl position-relative p-0">
            <nav class="navbar navbar-expand-lg navbar-dark bg-dark px-4 px-lg-5 py-3 py-lg-0">
                <a href="" class="navbar-brand p-0">
                    <!-- <h1 class="text-primary m-0"><i class="fa fa-utensils me-3"></i>IIP-XDU</h1> -->
                    <img src="img/logo.png" alt="Logo" >
                </a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse">
                    <span class="fa fa-bars"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarCollapse">
                    <div class="navbar-nav ms-auto py-0 pe-4">
                        <!-- <a href="https://opentvi.github.io/index.html" class="nav-item nav-link">Home</a>
                        <a href="https://opentvi.github.io/team.html" class="nav-item nav-link">Team</a>
                        <a href="https://opentvi.github.io/research.html" class="nav-item nav-link">Research</a>
                        <a href="https://opentvi.github.io/resources.html" class="nav-item nav-link active">Resources</a> -->
                        <!--
                        <div class="nav-item dropdown">
                            <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown">Projects</a>
                            <div class="dropdown-menu m-0">
                                <a href="booking.html" class="dropdown-item">XAI</a>
                                <a href="team.html" class="dropdown-item">AIGC</a>
                                <a href="testimonial.html" class="dropdown-item">HIT</a>
                            </div>
                        </div>
                      -->
                         
                    </div>
                    <!-- <a href="" class="nav-item nav-link"><i class="fa fa-2x fa-language text-primary "></i></a> -->
                </div>
            </nav>

            <div class="container-xxl py-5 bg-dark hero-header mb-5">
                <div class="container text-center my-5 pt-5 pb-4">
                    <h1 class="display-3 text-white mb-3 animated slideInDown">Shuang Li / Publications</h1>
                    <nav aria-label="breadcrumb">
                        <ol class="breadcrumb justify-content-center text-uppercase">
                            <li class="breadcrumb-item"><a href="index.html">Homepage</a></li>
                            <!-- <li class="breadcrumb-item"><a href="https://github.com/OpenTVI/OpenTVI.github.io">Github</a></li> -->
                            <li class="breadcrumb-item text-white active" aria-current="page">Publications</li>
                        </ol>
                    </nav>
                </div>
            </div>
        </div>
        <!-- Navbar & Hero End -->

        <!-- About Start -->
        <div class="container-xxl py-5">
            <div class="container">
                <div class="row g-5 align-items-center">

                    <div class="col-lg-8-1">
                        <h5 class="section-title ff-secondary text-start text-primary fw-normal">Publications</h5>
                        <p class="mb-4">Most of my publications explore human-centered trustworthy video surveillance systems. The complete collection is available on the <a href="https://scholar.google.com/citations?user=ePe0rG4AAAAJ&hl=zh-CN" target="_blank">Google Scholar profile</a>.</p>
                    
                    </div>
                </div>
            </div>
        </div>
        <!-- About End -->

         
        <!-- Menu Start -->

        <div class="container-xxl py-5">
            
            <div class="container">
                
              <!--
                <div class="text-center wow fadeInUp" data-wow-delay="0.1s">
                    <h5 class="section-title ff-secondary text-center text-primary fw-normal">Resources</h5>
                    <h1 class="mb-5">Most Popular Items</h1>
                </div>
                -->
                <div class="tab-class text-center wow fadeInUp" data-wow-delay="0.1s">
                        

                    <div class="tab-content">

                        <div id="tab-1" class="tab-pane fade show p-0 active">
                            <div class="row g-4">
                                <!--Âçï‰∏™ËÆ∫ÊñáÂºÄÂßã1-->
                                <h5 class=" ff-secondary   " style="font-size: 30px">2025</h5>
                                <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                                <div class="col-lg-12">
                                    <div class="paper-card p-3">
                                        <div class="d-flex align-items-start">
                                            <!-- ËÆ∫ÊñáÂõæÁâá -->
                                            <div class="col-lg-3 col-sm-3">
                                                <img src="./paper_img/ScRL.png" alt="Zoom Image" class="zoom-image">
                                            </div>

                                            <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                            <div class="w-100 d-flex flex-column text-start ps-4">
                                                <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                                <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                    <span style="font-weight: bold;">Shape-centered Representation Learning for Visible-Infrared Person Re-identification</span>
                                                    <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/10930619" target="_blank">
                                                        <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                    </a>
                                                </h5>

                                                <!-- ‰ΩúËÄÖÂàóË°® -->
                                                <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                    <strong>Shuang Li</strong>, Jiaxu Leng, Ji Gan, Mengjingcheng Mo, Xinbo Gaoüìß
                                                </p>

                                                <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                                <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                    Pattern Recognition (PR), 2025
                                                </p>

                                                <!-- ÂàÜÂâ≤Á∫ø -->
                                                <hr class="divider" style="margin: 8px 0;">

                                                <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                                <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                    Visible-Infrared Person Re-Identification (VI-ReID) plays a critical role in all-day surveillance systems. However, existing methods primarily focus on learning appearance features while overlooking body shape features, which not only complement appearance features but also exhibit inherent robustness to modality variations. Despite their potential, effectively integrating shape and appearance features remains challenging. Appearance features are highly susceptible to modality variations and background noise, while shape features often suffer from inaccurate infrared shape estimation due to the limitations of auxiliary models.
                                                </p>

                                                <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                    To address these challenges, we propose the Shape-centered Representation Learning (ScRL) framework, which enhances VI-ReID performance by innovatively integrating shape and appearance features. Specifically, we introduce Infrared Shape Restoration (ISR) to restore inaccuracies in infrared body shape representations at the feature level by leveraging infrared appearance features. In addition, we propose Shape Feature Propagation (SFP), which enables the direct extraction of shape features from original images during inference with minimal computational complexity. Furthermore, we design Appearance Feature Enhancement (AFE), which utilizes shape features to emphasize shape-related appearance features while effectively suppressing identity-unrelated noise.
Benefiting from the effective integration of shape and appearance features, ScRL demonstrates superior performance through extensive experiments.
                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                 <!--Âçï‰∏™ËÆ∫ÊñáÂºÄÂßã1-->


                                
                                <!--Âçï‰∏™ËÆ∫ÊñáÂºÄÂßã1-->
                                <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                                <div class="col-lg-12">
                                    <div class="paper-card p-3">
                                        <div class="d-flex align-items-start">
                                            <!-- ËÆ∫ÊñáÂõæÁâá -->
                                            <div class="col-lg-3 col-sm-3">
                                                <img src="./paper_img/xu_tim_2025.png" alt="Zoom Image" class="zoom-image">
                                            </div>

                                            <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                            <div class="w-100 d-flex flex-column text-start ps-4">
                                                <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                                <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                    <span style="font-weight: bold;">Mutual Information-guided Domain Shared Feature Learning for Bearing Fault Diagnosis under Unknown Conditions</span>
                                                    <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/10930619" target="_blank">
                                                        <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                    </a>
                                                </h5>

                                                <!-- ‰ΩúËÄÖÂàóË°® -->
                                                <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                    Kaixiong Xu, <strong>Shuang Li</strong>, Shuiqing Xu, Youqiang Hu, Yongfang Mao, Yi Chaiüìß
                                                </p>

                                                <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                                <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                    IEEE Transactions on Instrumentation and Measurement (IEEE TIM), 2025
                                                </p>

                                                <!-- ÂàÜÂâ≤Á∫ø -->
                                                <hr class="divider" style="margin: 8px 0;">

                                                <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                                <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                    With the support of unsupervised domain adaptation techniques, variable condition bearing fault diagnosis has achieved considerable progress. Nevertheless, the prerequisite of obtaining target data in advance limits the practical application of these diagnostic models in real-world scenarios. For bearing fault diagnosis under unknown conditions, domain generalization-based methods show great promise, and acquiring domain-invariant knowledge is crucial to enhance generalization capability.
                                                </p>

                                                <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                    To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                 <!--Âçï‰∏™ËÆ∫ÊñáÂºÄÂßã1-->


                                <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                                <div class="col-lg-12">
                                    <div class="paper-card p-3">
                                        <div class="d-flex align-items-start">
                                            <!-- ËÆ∫ÊñáÂõæÁâá -->
                                            <div class="col-lg-3 col-sm-3">
                                                <img src="./paper_img/dsreid.png" alt="Zoom Image" class="zoom-image">
                                            </div>

                                            <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                            <div class="w-100 d-flex flex-column text-start ps-4">
                                                <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                                <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                    <span style="font-weight: bold;">Dual-Space Video Person Re-identification</span>
                                                    <a class="btn btn-square btn-primary mx-1" href="https://link.springer.com/article/10.1007/s11263-025-02350-5" target="_blank">
                                                        <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                    </a>
                                                </h5>

                                                <!-- ‰ΩúËÄÖÂàóË°® -->
                                                <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                    Jiaxu Leng, Changjiang Kuang, <strong>Shuang Li</strong>, Ji Gan, Haosheng Chen and Xinbo Gaoüìß
                                                </p>

                                                <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                                <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                    International Journal of Computer Vision (IJCV), 2025
                                                </p>

                                                <!-- ÂàÜÂâ≤Á∫ø -->
                                                <hr class="divider" style="margin: 8px 0;">

                                                <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                                <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                    Video person re-identification (VReID) aims to recognize individuals across video sequences. Existing methods primarily use Euclidean space for representation learning but struggle to capture complex hierarchical structures, especially in scenarios with occlusions and background clutter. In contrast, hyperbolic space, with its negatively curved geometry, excels at preserving hierarchical relationships and enhancing discrimination between similar appearances. Inspired by these, we propose Dual-Space Video Person Re-Identification (DS-VReID) to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features while also exploring the intrinsic hierarchical relations, thereby enhancing the discriminative capacity of the features. Specifically, we design the Dynamic Prompt Graph Construction (DPGC) module, which uses a pre-trained CLIP model with learnable dynamic prompts to construct 3D graphs that capture subtle changes and dynamic information in video sequences. Building upon this, we introduce the Hyperbolic Disentangled Aggregation (HDA) module, which addresses long-range dependency modeling by decoupling node distances and integrating adjacency matrices, capturing detailed spatial-temporal hierarchical relationships.
                                                </p>

                                                <!-- <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                    To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                                </p> -->
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                 <!--Âçï‰∏™ËÆ∫ÊñáÁªìÊùü-->

                                 
                                <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                                <div class="col-lg-12">
                                    <div class="paper-card p-3">
                                        <div class="d-flex align-items-start">
                                            <!-- ËÆ∫ÊñáÂõæÁâá -->
                                            <div class="col-lg-3 col-sm-3">
                                                <img src="./paper_img/yan_spl_2025.png" alt="Zoom Image" class="zoom-image">
                                            </div>

                                            <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                            <div class="w-100 d-flex flex-column text-start ps-4">
                                                <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                                <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                    <span style="font-weight: bold;">TriMatch: Triple Matching for Text-to-Image Person Re-Identification</span>
                                                    <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/10855499" target="_blank">
                                                        <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                    </a>
                                                </h5>

                                                <!-- ‰ΩúËÄÖÂàóË°® -->
                                                <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                    Shuanglin Yan, Neng Dongüìß, <strong>Shuang Li</strong>, Huafeng Li
                                                </p>

                                                <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                                <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                    IEEE Signal Processing Letters  (SPL), 2025
                                                </p>

                                                <!-- ÂàÜÂâ≤Á∫ø -->
                                                <hr class="divider" style="margin: 8px 0;">

                                                <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                                <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                    Text-to-image person re-identification (TIReID) is a cross-modal retrieval task that aims to retrieve target person images based on a given text description. Existing methods primarily focus on mining the semantic associations across modalities, relying on the matching between heterogeneous features for retrieval. However, due to the inherent heterogeneous gaps between modalities, it is challenging to establish precise semantic associations, particularly in fine-grained correspondences, often leading to incorrect retrieval results. To address this issue, this letter proposes an innovative Triple Matching (TriMatch) framework that integrates cross-modal (image-text) matching and unimodal (image-image, text-text) matching for high-precision person retrieval. The framework introduces a generation task that performs cross-modal (image-to-text and text-to-image) feature generation and intra-modal feature alig achieve unimodal matching. By incorporating the generation task, TriMatch considers not only the semantic correlations between modalities but also the semantic consistency within single modalities, thereby effectively enhancing the accuracy of target person retrieval.
                                                </p>

                                                <!-- <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                    To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                                </p> -->
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                 <!--Âçï‰∏™ËÆ∫ÊñáÁªìÊùü-->
                                 
                                <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                                <div class="col-lg-12">
                                    <div class="paper-card p-3">
                                        <div class="d-flex align-items-start">
                                            <!-- ËÆ∫ÊñáÂõæÁâá -->
                                            <div class="col-lg-3 col-sm-3">
                                                <img src="./paper_img/LILING_IOT_2025.png" alt="Zoom Image" class="zoom-image">
                                            </div>

                                            <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                            <div class="w-100 d-flex flex-column text-start ps-4">
                                                <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                                <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                    <span style="font-weight: bold;">See as You Desire: Scale-Adaptive Face Super-Resolution for Varying Low Resolutions</span>
                                                    <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/10745550" target="_blank">
                                                        <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                    </a>
                                                </h5>

                                                <!-- ‰ΩúËÄÖÂàóË°® -->
                                                <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                    Ling Li, Yan Zhang, Lin Yuan, <strong>Shuang Li</strong>, Huafeng Li, Xinbo Gaoüìß
                                                </p>

                                                <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                                <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                    IEEE Internet of Things Journal (IEEE IOT), 2025
                                                </p>

                                                <!-- ÂàÜÂâ≤Á∫ø -->
                                                <hr class="divider" style="margin: 8px 0;">

                                                <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                                <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                    Face super-resolution (FSR) is critical for bolstering intelligent security in Internet of Things (IoT) systems. Recent deep learning-driven FSR algorithms have attained remarkable progress. However, they always require separate model training and optimization for each scaling factor or input resolution, leading to inefficiency and impracticality. To overcome these limitations, we propose SAFNet, an innovative framework tailored for scale-adaptive FSR with arbitrary input resolution. SAFNet integrates scale information into representation learning to enable adaptive feature extraction and introduces dual-embedding attention to boost adaptive feature reconstruction. It leverages facial self-similarity and spatial-frequency collaboration to achieve precise scale-aware SR representations. This is attained through three key modules: 1) the scale adaption guidance unit (SAGU); 2) the scale-aware nonlocal self-similarity (SNLS) module; and 3) the spatial-frequency interactive modulation (SFIM) module. SAGU imports scaling factors using frequency encoding, SNLS exploits self-similarity to enrich feature representations, and SFIM incorporates spatial and frequency information to predict target pixel values adaptively. Comprehensive evaluations across four benchmark datasets reveal that SAFNet outperforms the second-best compared state-of-the-art (SOTA) method by about 0.2 dB/0.007 in PSNR/SSIM ( √ó4 on CelebA) with reduced 18.68%/42.64% computational complexity/time cost.
                                                </p>

                                                <!-- <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                    To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                                </p> -->
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                 <!--Âçï‰∏™ËÆ∫ÊñáÁªìÊùü-->
                                 <h5 class=" ff-secondary   " style="font-size: 30px">2024</h5>

                                 <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                                <div class="col-lg-12">
                                    <div class="paper-card p-3">
                                        <div class="d-flex align-items-start">
                                            <!-- ËÆ∫ÊñáÂõæÁâá -->
                                            <div class="col-lg-3 col-sm-3">
                                                <img src="./paper_img/ZHUzhiqin_tcsvt_2024.png" alt="Zoom Image" class="zoom-image">
                                            </div>

                                            <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                            <div class="w-100 d-flex flex-column text-start ps-4">
                                                <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                                <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                    <span style="font-weight: bold;">Small Object Detection Method Based on Global Multi-Level Perception and Dynamic Region Aggregation</span>
                                                    <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/10542220" target="_blank">
                                                        <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                    </a>
                                                </h5>

                                                <!-- ‰ΩúËÄÖÂàóË°® -->
                                                <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                    Zhiqin Zhu, Renzhong Zheng, Guanqiu Qiüìß, <strong>Shuang Li</strong>, Yuanyuan Li, Xinbo Gao
                                                </p>

                                                <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                                <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                    IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), 2024
                                                </p>

                                                <!-- ÂàÜÂâ≤Á∫ø -->
                                                <hr class="divider" style="margin: 8px 0;">

                                                <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                                <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                    In the field of object detection, detecting small objects is an important and challenging task. However, most existing methods tend to focus on designing complex network structures, lack attention to global representation, and ignore redundant noise and dense distribution of small objects in complex networks. To address the above problems, this paper proposes a small object detection method based on global multi-level perception and dynamic region aggregation. The method achieves accurate detection by dynamically aggregating effective features within a region while fully perceiving the features. This method mainly consists of two modules: global multi-level perception module and dynamic region aggregation module. In the global multi-level perception module, self-attention is used to perceive the global region, and its linear transformation is mapped through a convolutional network to increase the local details of global perception, thereby obtaining more refined global information. The dynamic region aggregation module, devised with a sparse strategy in mind, selectively interacts with relevant features. This design allows aggregation of key features of individual instances, effectively mitigating noise interference. Consequently, this approach addresses the challenges associated with densely distributed targets and enhances the model‚Äôs ability to discriminate on a fine-grained level.
                                                </p>

                                                <!-- <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                    To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                                </p> -->
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                 <!--Âçï‰∏™ËÆ∫ÊñáÁªìÊùü-->

                                 <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                                 <div class="col-lg-12">
                                    <div class="paper-card p-3">
                                        <div class="d-flex align-items-start">
                                            <!-- ËÆ∫ÊñáÂõæÁâá -->
                                            <div class="col-lg-3 col-sm-3">
                                                <img src="./paper_img/LVtianle_2024.png" alt="Zoom Image" class="zoom-image">
                                            </div>

                                            <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                            <div class="w-100 d-flex flex-column text-start ps-4">
                                                <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                                <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                    <span style="font-weight: bold;">MGRL: Mutual-Guidance Representation Learning for Text-to-Image Person Retrieval</span>
                                                    <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/10542220" target="_blank">
                                                        <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                    </a>
                                                </h5>

                                                <!-- ‰ΩúËÄÖÂàóË°® -->
                                                <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                    Tianle Lv, <strong>Shuang Li</strong>, Jiaxu Leng, Xinbo Gaoüìß
                                                </p>

                                                <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                                <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024
                                                </p>

                                                <!-- ÂàÜÂâ≤Á∫ø -->
                                                <hr class="divider" style="margin: 8px 0;">

                                                <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                                <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                    Text-to-image person retrieval aims to recognize target pedestrians based on specified text. Existing methods mainly obtain image and text features separately through distinct feature extractors, subsequently embedding them into a unified feature space and calculating their similarity. Despite great success, current methods still suffer from the lack of information interaction between images and text. To address this issue, we propose Mutual-guidance Representation Learning (MGRL) for text-to-image person retrieval, which captures the key features for matching via text-image information interaction. Accordingly, our MGRL consists of two customized modules: iterative text-guided feature extraction (ITFE) and vision-assisted specific mask complement (VSMC). Specifically, ITFE is first designed to extract the matching information between the text and the image concerning the local feature attention of the target pedestrians by iterative text guidance. Then, to further ensure the image features extracted by ITFE contain the text description, VSMC is designed to utilize the extracted image features to help complete masked text where the mask is difficult to complete with only unmasked text information.
                                                </p>

                                                <!-- <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                    To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                                </p> -->
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                 <!--Âçï‰∏™ËÆ∫ÊñáÁªìÊùü-->

                                 <h5 class=" ff-secondary   " style="font-size: 30px">2023</h5>
                                 <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                                 <div class="col-lg-12">
                                    <div class="paper-card p-3">
                                        <div class="d-flex align-items-start">
                                            <!-- ËÆ∫ÊñáÂõæÁâá -->
                                            <div class="col-lg-3 col-sm-3">
                                                <img src="./paper_img/logic_tnnls_2023.png" alt="Zoom Image" class="zoom-image">
                                            </div>

                                            <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                            <div class="w-100 d-flex flex-column text-start ps-4">
                                                <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                                <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                    <span style="font-weight: bold;">Logical Relation Inference and Multiview Information Interaction for Domain Adaptation Person Re-Identification</span>
                                                    <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/10148098" target="_blank">
                                                        <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                    </a>
                                                </h5>

                                                <!-- ‰ΩúËÄÖÂàóË°® -->
                                                <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                    <strong>Shuang Li</strong>, Fan Li, Jinxing Liüìß, Huafeng Liüìß, Bob Zhang, Dapeng Tao, Xinbo Gao
                                                </p>

                                                <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                                <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                    IEEE Transactions on Neural Networks and Learning Systems (IEEE TNNLS), 2023
                                                </p>

                                                <!-- ÂàÜÂâ≤Á∫ø -->
                                                <hr class="divider" style="margin: 8px 0;">

                                                <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                                <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                    Domain adaptation person re-identification (Re-ID) is a challenging task, which aims to transfer the knowledge learned from the labeled source domain to the unlabeled target domain. Recently, some clustering-based domain adaptation Re-ID methods have achieved great success. However, these methods ignore the inferior influence on pseudo-label prediction due to the different camera styles. The reliability of the pseudo-label plays a key role in domain adaptation Re-ID, while the different camera styles bring great challenges for pseudo-label prediction. To this end, a novel method is proposed, which bridges the gap of different cameras and extracts more discriminative features from an image. Specifically, an intra-to-intermechanism is introduced, in which samples from their own cameras are first grouped and then aligned at the class level across different cameras followed by our logical relation inference (LRI). Thanks to these strategies, the logical relationship between simple classes and hard classes is justified, preventing sample loss caused by discarding the hard samples. Furthermore, we also present a multiview information interaction (MvII) module that takes features of different images from the same pedestrian as patch tokens, obtaining the global consistency of a pedestrian that contributes to the discriminative feature extraction. Unlike the existing clustering-based methods, our method employs a two-stage framework that generates reliable pseudo-labels from the views of the intracamera and intercamera, respectively, to differentiate the camera styles, subsequently increasing its robustness.
                                                </p>

                                                <!-- <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                    To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                                </p> -->
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                 <!--Âçï‰∏™ËÆ∫ÊñáÁªìÊùü-->

                                <h5 class=" ff-secondary   " style="font-size: 30px">2022</h5>
        


                                <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                                <div class="col-lg-12">
                                    <div class="paper-card p-3">
                                        <div class="d-flex align-items-start">
                                            <!-- ËÆ∫ÊñáÂõæÁâá -->
                                            <div class="col-lg-3 col-sm-3">
                                                <img src="./paper_img/wangyongzeng_acmmm_2022.png" alt="Zoom Image" class="zoom-image">
                                            </div>

                                            <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                            <div class="w-100 d-flex flex-column text-start ps-4">
                                                <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                                <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                    <span style="font-weight: bold;">Cross-compatible embedding and semantic consistent feature construction for sketch re-identification</span>
                                                    <a class="btn btn-square btn-primary mx-1" href="https://dl.acm.org/doi/abs/10.1145/3503161.3548224" target="_blank">
                                                        <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                    </a>
                                                </h5>

                                                <!-- ‰ΩúËÄÖÂàóË°® -->
                                                <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                    Yafei Zhang, Yongzeng Wang, Huafeng Liüìß, <strong>Shuang Li</strong>üìß
                                                </p>

                                                <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                                <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                    ACM International Conference on Multimedia (ACM MM), 2022
                                                </p>

                                                <!-- ÂàÜÂâ≤Á∫ø -->
                                                <hr class="divider" style="margin: 8px 0;">

                                                <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                                <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                    Sketch re-identification (Re-ID) refers to using sketches of pedestrians to retrieve their corresponding photos from surveillance videos. It can track pedestrians according to the sketches drawn based on eyewitnesses without querying pedestrian photos. Although the Sketch Re-ID concept has been proposed, the gap between the sketch and the photo still greatly hinders pedestrian identity matching. Based on the idea of transplantation without rejection, we propose a Cross-Compatible Embedding (CCE) approach to narrow the gap. A Semantic Consistent Feature Construction (SCFC) scheme is simultaneously presented to enhance feature discrimination. Under the guidance of identity consistency, the CCE performs cross modal interchange at the local token level in the Transformer framework, enabling the model to extract modal-compatible features. The SCFC improves the representation ability of features by handling the inconsistency of information in the same location of the sketch and the corresponding pedestrian photo. The SCFC scheme divides the local tokens of pedestrian images with different modes into different groups and assigns specific semantic information to each group for constructing a semantic consistent global feature representation. Experiments on the public Sketch Re-ID dataset confirm the effectiveness of the proposed method and its superiority over existing methods.
                                                </p>

                                                <!-- <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                    To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                                </p> -->
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <!--Âçï‰∏™ËÆ∫ÊñáÁªìÊùü-->
        
                                <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                                <div class="col-lg-12">
                                    <div class="paper-card p-3">
                                        <div class="d-flex align-items-start">
                                            <!-- ËÆ∫ÊñáÂõæÁâá -->
                                            <div class="col-lg-3 col-sm-3">
                                                <img src="./paper_img/wangyiming_tifs_2022.png" alt="Zoom Image" class="zoom-image">
                                            </div>

                                            <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                            <div class="w-100 d-flex flex-column text-start ps-4">
                                                <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                                <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                    <span style="font-weight: bold;">Body Part-Level Domain Alignment for Domain-Adaptive Person Re-Identification With Transformer Framework</span>
                                                    <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/9895288" target="_blank">
                                                        <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                    </a>
                                                </h5>

                                                <!-- ‰ΩúËÄÖÂàóË°® -->
                                                <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                    Yiming Wang, Guanqiu Qi, <strong>Shuang Li</strong>üìß, Yi Chaiüìß, Huafeng Li
                                                </p>

                                                <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                                <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                    IEEE Transactions on Information Forensics and Security (IEEE TIFS), 2022
                                                </p>

                                                <!-- ÂàÜÂâ≤Á∫ø -->
                                                <hr class="divider" style="margin: 8px 0;">

                                                <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                                <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                    Although existing domain-adaptive person re-identification (re-ID) methods have achieved competitive per- formance, most of them highly rely on the reliability of pseudo-label prediction, which seriously limits their applicability as noisy labels cannot be avoided. This paper designs a Transformer framework based on body part-level domain alignment to solve the above-mentioned issues in domain-adaptive person re-ID. Different parts of the human body (such as head, torso, and legs) have different structures and shapes. Therefore, they usually exhibit different characteristics. The proposed method makes full use of the dissimilarity between different human body parts. Specifically, the local features from the same body part are aggregated by the Transformer to obtain the corresponding class token, which is used as the global representation of this body part. Additionally, a Transformer layer-embedded adversarial learning strategy is designed. This strategy can simultaneously achieve domain alignment and classification of the class token for each human body part in both target and source domains by an integrated discriminator, thereby realizing domain alignment at human body part level. Compared with existing domain-level and identity-level alignment methods, the proposed method has a stronger fine-grained domain alignment capability. Therefore, the information loss or distortion that may occur in the feature alignment process can be effectively alleviated. The proposed method does not need to predict pseudo labels of any target sample, so the negative impact caused by unreliable pseudo labels on re-ID performance can be effectively avoided.
                                                </p>

                                                <!-- <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                    To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                                </p> -->
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <!--Âçï‰∏™ËÆ∫ÊñáÁªìÊùü-->

                             <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                             <div class="col-lg-12">
                                <div class="paper-card p-3">
                                    <div class="d-flex align-items-start">
                                        <!-- ËÆ∫ÊñáÂõæÁâá -->
                                        <div class="col-lg-3 col-sm-3">
                                            <img src="./paper_img/shuangli_first.png" alt="Zoom Image" class="zoom-image">
                                        </div>

                                        <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                            <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                <span style="font-weight: bold;">Mutual prediction learning and mixed viewpoints for unsupervised-domain adaptation person re-identification on blockchain</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://www.sciencedirect.com/science/article/abs/pii/S1569190X22000594" target="_blank">
                                                    <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                </a>
                                            </h5>

                                            <!-- ‰ΩúËÄÖÂàóË°® -->
                                            <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                <strong>Shuang Li</strong>, Fan Li, Kunpeng Wangüìß, Guanqiu Qi, Huafeng Li
                                            </p>

                                            <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                            <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                Simulation Modelling Practice and Theory, 2022
                                            </p>

                                            <!-- ÂàÜÂâ≤Á∫ø -->
                                            <hr class="divider" style="margin: 8px 0;">

                                            <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                            <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                In addition to the domain shift between different datasets, the diversity of pedestrian appearance (physical appearance and postures) caused by different camera views also affects the performance of person re-identification (re-ID). Since existing methods tend to extract the shared information of the same pedestrian across multiple images, the above diversity issue has not been effectively alleviated. In addition, while making full use of pedestrian image data and realizing its value, there are also risks of privacy leakage and data loss. Therefore, this paper proposes the mutual prediction learning (MPL) and mixed viewpoints for unsupervised domain adaptation (UDA) person re-ID on blockchain. This method enables the network to first obtain the ability of MPL on multi-view polymorphic features and further acquire the reasoning imagination to alleviate the ambiguity caused by morphological differences. In the process of MPL, the training samples are first divided into different groups and each group has two sets. Then the corresponding identity classifiers of every two sets are integrated and applied to the cross-prediction of polymorphic features. Finally, the joint distribution alignment of domain- and identity-level features is achieved. Furthermore, an adversarial mechanism of mixed viewpoints is proposed to improve the accuracy of identity matching. The domain-invariant salient features are extracted and fused with the polymorphic features obtained by global average pooling (GAP) after domain alignment.
                                            </p>

                                            <!-- <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                            </p> -->
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <!--Âçï‰∏™ËÆ∫ÊñáÁªìÊùü-->


                             <!-- Âçï‰∏™ËÆ∫ÊñáÂç°Áâá -->
                             <div class="col-lg-12">
                                <div class="paper-card p-3">
                                    <div class="d-flex align-items-start">
                                        <!-- ËÆ∫ÊñáÂõæÁâá -->
                                        <div class="col-lg-3 col-sm-3">
                                            <img src="./paper_img/cvpr_vvireid.png" alt="Zoom Image" class="zoom-image">
                                        </div>

                                        <!-- ËÆ∫Êñá‰ø°ÊÅØ -->
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <!-- ËÆ∫ÊñáÊ†áÈ¢ò -->
                                            <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: 3px;">
                                                <span style="font-weight: bold;">Learning modal-invariant and temporal-memory for video-based visible-infrared person re-identification</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://openaccess.thecvf.com/content/CVPR2022/html/Lin_Learning_Modal-Invariant_and_Temporal-Memory_for_Video-Based_Visible-Infrared_Person_Re-Identification_CVPR_2022_paper.html" target="_blank">
                                                    <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                </a>
                                            </h5>

                                            <!-- ‰ΩúËÄÖÂàóË°® -->
                                            <p class="author-list" style="margin-top: 3px; margin-bottom: 3px; font-size: 14px; color: #555;">
                                                Xinyu Lin, Jinxing Liüìß, Zeyu Ma, Huafeng Li, <strong>Shuang Li</strong>,  Kaixiong Xu, Guangming Lu, David Zhang
                                            </p>

                                            <!-- ÊúüÂàä‰ø°ÊÅØ -->
                                            <p class="journal-info fst-italic" style="margin-top: 3px; margin-bottom: 8px; font-weight: bold; color: #333;">
                                                IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022
                                            </p>

                                            <!-- ÂàÜÂâ≤Á∫ø -->
                                            <hr class="divider" style="margin: 8px 0;">

                                            <!-- ËÆ∫ÊñáÊëòË¶Å -->
                                            <p style="font-size: 14px; line-height: 1.5; color: #444; margin-bottom: 8px;">
                                                Thanks for the cross-modal retrieval techniques, visible-infrared (RGB-IR) person re-identification (Re-ID) is achieved by projecting them into a common space, allowing person Re-ID in 24-hour surveillance systems. However, with respect to the "probe-to-gallery", almost all existing RGB-IR based cross-modal person Re-ID methods focus on image-to-image matching, while the video-to-video matching which contains much richer spatial- and temporal-information remains under-explored. In this paper, we primarily study the video-based cross-modal person Re-ID method. To achieve this task, a video-based RGB-IR dataset is constructed, in which 927 valid identities with 463,259 frames and 21,863 tracklets captured by 12 RGB/IR cameras are collected. Based on our constructed dataset, we prove that with the increase of frames in a tracklet, the performance does meet more enhancement, demonstrating the significance of video-to-video matching in RGB-IR person Re-ID. Additionally, a novel method is further proposed, which not only projects two modalities to a modal-invariant subspace, but also extracts the temporal-memory for motion-invariant.
                                            </p>

                                            <!-- <p style="font-size: 14px; line-height: 1.5; color: #444;">
                                                To address this issue, this paper proposes Mutual Information-guided Domain Shared Feature Learning algorithm (MI-DSFL). MI-DSFL designs a HS diagnosis branch and a WC identification branch to directly extract HS-related features and WC-related features, respectively. Through the interaction between the two branches and the mutual influence of the corresponding classifiers, domain-invariant HS-related features and WC-related features are ultimately decoupled.
                                            </p> -->
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <!--Âçï‰∏™ËÆ∫ÊñáÁªìÊùü-->
                            </div>
                        </div>

                        <div id="tab-2" class="tab-pane fade show p-0">
                            <div class="row g-4">
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-center">
                                        <div class="col-lg-3 col-sm-3">
                                            <img class="flex-shrink-0 img-fluid rounded" src="img/bg-hero_van.jpg" alt=""">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Chicken Burger</span>
                                                <a class="btn btn-square btn-primary mx-1" href="" target="_blank"><i class="fab fa-github"></i></a>
                                            </h5>
                                            <small class="fst-italic">Ipsum ipsum clita erat amet dolor justo diam</small>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-center">
                                        <div class="col-lg-3 col-sm-3">
                                            <img class="flex-shrink-0 img-fluid rounded" src="img/bg-hero_van.jpg" alt=""">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Chicken Burger</span>
                                                <a class="btn btn-square btn-primary mx-1" href="" target="_blank"><i class="fab fa-github"></i></a>
                                            </h5>
                                            <small class="fst-italic">Ipsum ipsum clita erat amet dolor justo diam</small>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div id="tab-3" class="tab-pane fade show p-0">
                            <div class="row g-4">
                                <div class="col-lg-6">
                                    <div class="d-flex align-items-center">
                                        <img class="flex-shrink-0 img-fluid rounded" src="img/applet.png" alt="" style="width: 120px;">
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Weixin Applet</span>
                                                <span class="text-primary"><i class="fab fa-weixin"></i></span>
                                            </h5>
                                            <small class="fst-italic">Ipsum ipsum clita erat amet dolor justo diam</small>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-6">
                                    <div class="d-flex align-items-center">
                                        <img class="flex-shrink-0 img-fluid rounded" src="img/bird.jpg" alt="" style="width: 80px;">
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Chicken Burger</span>
                                                <span class="text-primary">$115</span>
                                            </h5>
                                            <small class="fst-italic">Ipsum ipsum clita erat amet dolor justo diam</small>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-6">
                                    <div class="d-flex align-items-center">
                                        <img class="flex-shrink-0 img-fluid rounded" src="img/bird.jpg" alt="" style="width: 80px;">
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Chicken Burger</span>
                                                <span class="text-primary">$115</span>
                                            </h5>
                                            <small class="fst-italic">Ipsum ipsum clita erat amet dolor justo diam</small>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>



                        <div id="tab-4" class="tab-pane fade show p-0">
                            <div class="row g-4">
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-center">
                                        <div class="col-lg-3 col-sm-3">
                                            <img class="flex-shrink-0 img-fluid rounded" src="img/iip_black_h.png" alt=""">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>IIP Logos</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://github.com/OpenTVI/OpenTVI.github.io" target="_blank"><i class="fab fa-github"></i></a>
                                            </h5>
                                            <small class="fst-italic">Different versions of our logos, for uses under different scenarios.  </small>
                                            <a class="btn btn-square btn-primary mx-1" href="resources/iip-logo.zip"><i class="fa fa-download"></i></a>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-center">
                                        <div class="col-lg-3 col-sm-3">
                                            <img class="flex-shrink-0 img-fluid rounded" src="img/xdu_red_black.png" alt="">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>XDU Logos</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://github.com/note286/xdulogo" target="_blank"><i class="fab fa-github"></i></a>
                                            </h5>
                                            <small class="fst-italic">Different versions of XDU logos, for uses under different scenarios.  </small>
                                            <a class="btn btn-square btn-primary mx-1" href="resources/xdu-logo.zip"><i class="fa fa-download"></i></a>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-center">
                                        <div class="col-lg-3 col-sm-3">
                                            <img class="flex-shrink-0 img-fluid rounded" src="img/ppt-iip.png" alt="">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Templetes</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://github.com/iip-xdu/iip-template" target="_blank"><i class="fab fa-github"></i></a>
                                            </h5>
                                            <small class="fst-italic"> Slides templates (16:9 PPT) with our logos.</small>
                                            <a class="btn btn-square btn-primary mx-1" href="https://github.com/iip-xdu/iip-template" target="_blank"><i class="fa fa-download"></i></a>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>


                    </div>
                </div>
            </div>
        </div>
        <!-- Menu End -->


        <!-- Footer Start -->
        <div class="container-fluid bg-dark text-light footer pt-5 mt-5 wow fadeIn" data-wow-delay="0.1s">
            <div class="container py-5">
                <div class="row g-5">
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Laboratory</h4>
                        <a class="btn btn-link" href="">About Us</a>
                        <a class="btn btn-link" href="">Contact Us</a>
                        <a class="btn btn-link" href="">Reservation</a>
                        <a class="btn btn-link" href="">Privacy Policy</a>
                        <a class="btn btn-link" href="">Terms & Condition</a>
                    </div>
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Contact</h4>
                        <p class="mb-2"><i class="fa fa-map-marker-alt me-3"></i>No. 2 Chongwen Road, Nan'an District, Chongqing, </p>
                        <p class="mb-2"><i class="fa fa-globe me-3"></i>CQUPT, ChongQing 10617, China</p>
                        <p class="mb-2"><i class="fa fa-envelope me-3"></i>lengjx <i class="fa fa-at"></i> cqupt.edu.cn</p>
                        <div class="d-flex pt-2">
                            <a class="btn btn-outline-light btn-social" href=""><i class="fab fa-weixin"></i></a>
                            <a class="btn btn-outline-light btn-social" href="https://github.com/OpenTVI/OpenTVI.github.io" target="_blank"><i class="fab fa-github"></i></a>
                            <!--
                            <a class="btn btn-outline-light btn-social" href=""><i class="fab fa-youtube"></i></a>
                            <a class="btn btn-outline-light btn-social" href=""><i class="fab fa-linkedin-in"></i></a>
                          -->
                        </div>
                    </div>
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Links</h4>
                        <a class="btn btn-link" href="https://www.cqupt.edu.cn/" target="_blank">CQUPT</a>
                        <a class="btn btn-link" href="https://see.xidian.edu.cn/faculty/xbgao/" target="_blank">VIPSL (Xinbo Gao) </a>
                        <a class="btn btn-link" href="https://www.researchgate.net/profile/Jiaxu-Leng-2" target="_blank">Home (Jiaxu Leng) </a>
                    </div>
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Pages</h4>
                        <!--
                        <p>You can try our work.</p>
                        <div class="position-relative mx-auto" style="max-width: 400px;">
                            <input class="form-control border-primary w-100 py-3 ps-4 pe-5" type="text" placeholder="Your email">
                            <button type="button" class="btn btn-primary py-2 position-absolute top-0 end-0 mt-2 me-2">SignUp</button>
                        </div>
                        <img class="img-fluid flex-shrink-0 rounded-circle" src="img/applet.png" style="width: 50px; height: 50px;">
                      -->
                        <a class="btn btn-link" href="https://opentvi.github.io/index.html" class="nav-item nav-link">Home</a>
                        <a class="btn btn-link" href="https://opentvi.github.io/team.html" class="nav-item nav-link">Team</a>
                        <a class="btn btn-link" href="https://opentvi.github.io/research.html" class="nav-item nav-link">Research</a>
                        <a class="btn btn-link" href="https://opentvi.github.io/resources.html" class="nav-item nav-link">Resources</a>
                        <a class="btn btn-link" href="https://opentvi.github.io/contact.html" class="nav-item nav-link">Contact</a>
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="copyright">
                    <div class="row">
                        <div class="col-md-6 text-center text-md-start mb-3 mb-md-0">
                            &copy; <a class="border-bottom" href="#">TVI Group.</a>, All Right Reserved.

              <!--/*** This template is free as long as you keep the footer author‚Äôs credit link/attribution link/backlink. If you'd like to use the template without the footer author‚Äôs credit link/attribution link/backlink, you can purchase the Credit Removal License from "https://htmlcodex.com/credit-removal". Thank you for your support. ***/-->
              Designed By <a class="border-bottom" href="https://htmlcodex.com">HTML Codex</a>
                        </div>
                        <div class="col-md-6 text-center text-md-end">
                            <div class="footer-menu">
                                <a href="">Home</a>
                                <a href="">Cookies</a>
                                <a href="">Help</a>
                                <a href="">FQAs</a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Footer End -->


        <!-- Back to Top -->
        <a href="#" class="btn btn-lg btn-primary btn-lg-square back-to-top"><i class="bi bi-arrow-up"></i></a>
    </div>

    <!-- JavaScript Libraries -->
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="lib/wow/wow.min.js"></script>
    <script src="lib/easing/easing.min.js"></script>
    <script src="lib/waypoints/waypoints.min.js"></script>
    <script src="lib/counterup/counterup.min.js"></script>
    <script src="lib/owlcarousel/owl.carousel.min.js"></script>
    <script src="lib/tempusdominus/js/moment.min.js"></script>
    <script src="lib/tempusdominus/js/moment-timezone.min.js"></script>
    <script src="lib/tempusdominus/js/tempusdominus-bootstrap-4.min.js"></script>

    <!-- Template Javascript -->
    <script src="js/main.js"></script>
</body>

</html>
